
import com.amazon.deequ.{VerificationSuite, VerificationResult}
import com.amazon.deequ.VerificationResult.checkResultsAsDataFrame
import com.amazon.deequ.checks.{Check, CheckLevel}
import com.amazon.deequ.suggestions.{ConstraintSuggestionRunner, Rules}
import com.amazon.deequ.profiles.{ColumnProfilerRunner, NumericColumnProfile}
import com.amazon.deequ.schema
import com.amazon.deequ.schema.{RowLevelSchema,RowLevelSchemaValidator}

var demo = """


BasicEvaluation
StatisticalEvaluation

ReportViolations


var suggestions = SuggestConstraints()
suggestions.select("_1", "_2").show(100, 50)


showEvaluation
EvaluateDataQuality

"""

val args = sc.getConf.get("spark.driver.args").split("\\s+")
val source_bucket=args(0)
val output_bucket=args(1)

// println("Source Bucket : " + source_bucket)
// println("Output Bucket : " + output_bucket)


val dataset = spark.read.option("header","true").option("inferSchema", "true").csv(source_bucket)

// dataset.show(truncate=true)
dataset.printSchema



def BasicEvaluation(): Unit = {
	val result = ColumnProfilerRunner().onData(dataset).run()

	result.profiles.foreach { case (productName, profile) =>
	  println(s"Column '$productName':\n " +
	    s"\tcompleteness: ${profile.completeness}\n" +
	    // s"\tapproximate number of distinct values: ${profile.approximateNumDistinctValues}\n" +
	    s"\tdatatype: ${profile.dataType}\n")
	}
}


def StatisticalEvaluation(): Unit = {

	val result = ColumnProfilerRunner().onData(dataset).run()

	val countProfile = result.profiles("dti").asInstanceOf[NumericColumnProfile]

	println(s"Statistics of 'dti' Debt-To-Income Ratio:\n" +
	  s"\tminimum: ${countProfile.minimum.get}\n" +
	  s"\tmaximum: ${countProfile.maximum.get}\n" +
	  s"\tmean: ${countProfile.mean.get}\n" +
	  s"\tstandard deviation: ${countProfile.stdDev.get}\n")

	val recProfile = result.profiles("interest_rate").asInstanceOf[NumericColumnProfile]

	println(s"Statistics of 'interest_rate' Interest Rate:\n" +
	  s"\tminimum: ${recProfile.minimum.get}\n" +
	  s"\tmaximum: ${recProfile.maximum.get}\n" +
	  s"\tmean: ${recProfile.mean.get}\n" +
	  s"\tstandard deviation: ${recProfile.stdDev.get}\n")

}

def SuggestConstraints(): org.apache.spark.sql.Dataset[(String, String, String)] = {
	val suggestionResult = { ConstraintSuggestionRunner().onData(dataset).addConstraintRules(Rules.DEFAULT).run()}
	suggestionResult.constraintSuggestions.flatMap { 
	  case (column, suggestions) => 
	    suggestions.map { constraint =>
	      (column, constraint.description, constraint.codeForConstraint)
	    } 
	}.toSeq.toDS()
		
}
var suggestions = SuggestConstraints()
// suggestions.coalesce(1).write.mode("overwrite").format("com.databricks.spark.csv").option("header", "true").save(output_bucket)
/////////////////////////////////////////////////////////////////////////////////////////

def showEvaluation = """
def EvaluateDataQuality(): Unit = {

	
	val verificationResult: VerificationResult = { VerificationSuite().onData(dataset).addCheck(
		Check(CheckLevel.Error, "Review Check")
		
		.isUnique("id")

		.isComplete("purpose_cat")
		.isComplete("annual_inc")
		.isComplete("total_pymnt")
		.isComplete("grade_cat")

		.isComplete("interest_rate") 
		.isNonNegative("interest_rate")

		.hasMax("loan_amount", _ == 5000)
		
		.isContainedIn("term", Array(" 36 months", " 60 x	months"))
		.isContainedIn("loan_condition", Array("Good Loan"), _ >= 0.95, Some("It should be above 0.95!"))                                 

		).run() 
	}
"""


def EvaluateDataQuality(): Unit = {

	
	val verificationResult: VerificationResult = { VerificationSuite().onData(dataset).addCheck(
		Check(CheckLevel.Error, "Review Check")
		
		.isUnique("id")

		.isComplete("purpose_cat")
		.isComplete("annual_inc")
		.isComplete("total_pymnt")
		.isComplete("grade_cat")

		.isComplete("interest_rate") 
		.isNonNegative("interest_rate")

		.hasMax("loan_amount", _ == 5000)
		
		.isContainedIn("term", Array(" 36 months", " 60 months"))
		.isContainedIn("loan_condition", Array("Good Loan"), _ >= 0.95, Some("It should be above 0.95!"))                                 

		).run() 
	}

	val resultDataFrame = checkResultsAsDataFrame(spark, verificationResult)

	// resultDataFrame.show(20, 100)
	resultDataFrame.select("constraint", "constraint_status", "constraint_message").show()
	resultDataFrame.select("constraint_message").show(100,100)


	sc.hadoopConfiguration.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")
	sc.hadoopConfiguration.set("parquet.enable.summary-metadata", "false")
	resultDataFrame.coalesce(1).write.mode("overwrite").format("com.databricks.spark.csv").option("header", "true").save(output_bucket)
}



def ReportViolations(): Unit = {

	val schema = RowLevelSchema().withIntColumn("loan_amount", isNullable = false)
	val result = RowLevelSchemaValidator.validate(dataset, schema)

	print("Number of failed rows: ")
	println(result.numInvalidRows)
	println("Rows: ")
	result.invalidRows.show(truncate=true)
}


